---
title: "using_pubchunks_ADC_citations"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

#install.packages("pubchunks")
library(pubchunks)
#install.packages("fulltext")
library(fulltext)

library(arcticdatautils)
library(dataone)
library(datapack)
library(EML)
library(remotes)
library(XML)
library(datamgmt)
library(tidyverse)
library(here)
#install.packages("reprex")
library(reprex)
library(tibble)
library(readr)
library(dplyr)
library(xlsx)
library(stringr)
library(text2vec)

```

Getting a list of ADC grant award numbers to use as a query
```{r}

 cn <- CNode("PROD")
    mn <- getMNode(cn, "urn:node:ARCTIC")

    result <-
        query(mn, list(
                    q = "formatType:METADATA AND (*:* NOT obsoletedBy:*)",
                    fl = "identifier,rightsHolder,formatId, funding, abstract",
                    start = "0",
                    rows = "15000"),
                as = "data.frame"
        )
    dois <- grep("doi", result$identifier, value = T) %>%
        gsub("doi:", "", .data$.)

#Isolating the grant numbers
funding_and_doi <- result %>%
  select(identifier, funding, abstract)

#Just keeping the numbers, removing any extra text
funding_and_doi$funding <- as.numeric(gsub("([0-9]+).*$", "\\1", funding_and_doi$funding))

ADC_funding_and_doi <- na.omit(funding_and_doi)

#Seeing how many unique award number their are
unique(ADC_funding_and_doi$funding)

#renaming the column so I don't get confused about what it is
ADC_funding_and_doi <- ADC_funding_and_doi%>% 
  rename("ADC_doi" = "identifier")

```
```{r}


awards <- unique(ADC_funding_and_doi$funding)
# create an empty list to store results in
pub <- list()
  for (i in seq_along(awards)){
  print_fields <- 'id,date,startDate,fundProgramName,poName,title,awardee,piFirstName,piLastName,publicationResearch,publicationConference,abstractText'
  query_url <- paste0("https://api.nsf.gov/services/v1/awards.json?id=", awards[i], '&printFields=', print_fields)
  json_result <- jsonlite::fromJSON(query_url)
  
   if (is.null(json_result$response$award$publicationResearch))
            pub[[i]] <- data.frame(Pub_citation = NA,
                                            funding = awards[i], 
                                   pub_abstract = NA) 
  
  else if (!is.null(json_result$response$award$publicationResearch)) {
     pub[[i]] <- data.frame(Pub_citation = json_result$response$award$publicationResearch,
                                            funding = awards[i], pub_abstract = json_result$response$award$abstractText)  
    
     colnames(pub[[i]]) <- c("Pub_citation", "funding", "pub_abstract")
    
  }}



dfs <- lapply(pub , data.frame, stringsAsFactors = FALSE)
pub_df <- plyr::rbind.fill(dfs)

#Making sure the class is numeric so it can be used as a key to join tables
#pub$funding <- as.numeric(json_result$response$award$id)

```


Joining all of the award qury outputs with the ADC info and then merging those

```{r}

full_table1 <- full_join(ADC_funding_and_doi, pub_df, by = 'funding')

#ended up with some duplicate rows. Removing them here
full_table <- full_table[!duplicated(full_table1$Pub_citation), ]

```

Save as a csv

```{r}


write.csv(full_table,"ADC_pubs_KPEACH.csv", row.names = FALSE)

```

Clean up abstracts to prepare them for text similarity analysis
```{r}


prep_fun = function(x) {
    # make text lower case
    x = str_to_lower(x)
    # remove non-alphanumeric symbols
    x = str_replace_all(x, "[^[:alnum:]]", " ")
    # collapse multiple spaces
    str_replace_all(x, "\\s+", " ")
}

full_table$abstract <- prep_fun(full_table$abstract)

full_table$pub_abstract <- prep_fun(full_table$pub_abstract)

```

Julien said that sometimes datasets have exactly the same abstract so I thought maybe agrep would work but that must not be super common practice

```{r}

agrep(full_table$abstract[2], full_table$pub_abstract, ignore.case=T,value=T,max.distance = 0.1, useBytes = FALSE)

```


Making two corpuses (?) 

```{r}

#A corpus of ADc dataset dois
corp1 = quanteda::corpus(full_table, text_field= 'abstract')
dtm1 = dfm(corp1)

dtm1

#A corpus of pub abstracts
corp2 = quanteda::corpus(full_table, docid_field = 'Pub_citation', text_field= 'pub_abstract')
dtm2 = dfm(corp2)

dtm2


```




Define two sets of documents to compare

```{r}

it1 <- itoken(full_table$abstract, progressbar = FALSE)


it2 <- itoken(full_table$pub_abstract, progressbar = FALSE)

```

We will compare documents in a vector space. So we need to define common space and project documents to it. We will use vocabulary-based vectorization vectorization for better interpretability:

```{r}

it = itoken(full_table$pub_abstract, progressbar = FALSE)
v = create_vocabulary(it)
v = prune_vocabulary(v, doc_proportion_max = 0.1, term_count_min = 5)
vectorizer = vocab_vectorizer(v)


```

Jaccard similarity is a simple but intuitive measure of similarity between two sets. For documents we measure it as proportion of number of common words to number of unique words in both documents. In the field of NLP jaccard similarity can be particularly useful for duplicates detection. text2vec however provides generic efficient realization which can be used in many other applications.

For calculation of jaccard similarity between 2 sets of documents user have to provide DTM for each them (DTMs should be in the same vector space!):
```{r}

# they will be in the same space because we use same vectorizer
# hash_vectorizer will also work fine
dtm1 = create_dtm(it1, vectorizer)
dim(dtm1)

dtm2 = create_dtm(it2, vectorizer)
dim(dtm2)

d1_d2_jac_sim = sim2(dtm1, dtm2, method = "jaccard", norm = "none")

dim(d1_d2_jac_sim)

dtm1_2 = dtm1[1:200, ]
dtm2_2 = dtm2[1:200, ]
d1_d2_jac_psim = psim2(dtm1_2, dtm2_2, method = "jaccard", norm = "none")
str(d1_d2_jac_psim)

```

```{r}

d1_d2_cos_sim = sim2(dtm1, dtm2, method = "cosine", norm = "l2")

dim(d1_d2_cos_sim)

```


Getting a list of data package DOI's from the ADC

```{r}

 cn <- CNode("PROD")
    mn <- getMNode(cn, "urn:node:ARCTIC")

    result <-
        query(mn, list(
                    q = "formatType:METADATA AND (*:* NOT obsoletedBy:*)",
                    fl = "identifier,rightsHolder,formatId",
                    start = "0",
                    rows = "15000"),
                as = "data.frame"
        )
    dois <- grep("doi", result$identifier, value = T) %>%
        gsub("doi:", "", .data$.)

dois <- result$identifier 

#Took a random sample of dois from the list

doi_100_sample <- sample(dois, 100)

doi_10_sample <- sample(dois, 10)


```

Queried several databases for publications that include any of the dois in my sample. Also queried single dois and did not find any results

```{r}

res <- ft_search(query = 'doi:10.5065/D6M043F3', from=c('plos','crossref','arxiv', 'biorxivr', 'europe_pmc', 'ma'), limit = 10)

articles <- ft_get(res$plos$data$id)

titles <- pub_chunks(fulltext::ft_collect(articles), sections="title")


```


Simple ADC Search

To find documents where your search terms appear adjacent to each other, enclose the terms in double quotation marks: "cell behaviour". When you use double quotation marks AND is not automatically inserted between terms.

```{r, message = FALSE}

res1 <- ft_search(query = "Arctic Data Center", from=c('plos','crossref','arxiv', 'biorxivr', 'europe_pmc', 'ma'), limit = 1000)

plos_articles <- ft_get(res1$plos$data$id[1:1000])

plos_titles <- pub_chunks(fulltext::ft_collect(plos_articles), sections= c("doi", "title", "authors"))

plos_table <- pub_tabularize(plos_titles, bind = TRUE)

plos_table <- plos_table %>% select(doi, title, authors.given_names, authors.surname, authors.given_names.1, authors.surname.1, authors.given_names.2, authors.surname.2, authors.given_names.3, authors.surname.3, .publisher)

write.xlsx(plos_table, file = "plos_table.xlsx", sheetName = "Sheet1", 
  col.names = TRUE, row.names = FALSE, append = FALSE)

```



```{r}

res2 <- ft_search(query = 'arcticdata.io', from=c('plos','crossref','arxiv', 'biorxivr', 'europe_pmc', 'ma'), limit = 100)

plos_articles_io <- ft_get(res2$plos$data$id)

plos_titles_io <- pub_chunks(fulltext::ft_collect(plos_articles_io), sections= c("doi", "title", "authors"))

plos_table_io <- pub_tabularize(plos_titles_io, bind = TRUE)

plos_table_io <- plos_table_io %>% select(doi, title, authors.given_names, authors.surname, authors.given_names.1, authors.surname.1, authors.given_names.2, authors.surname.2, authors.given_names.3, authors.surname.3, .publisher)


write.xlsx(plos_table_io, file = "plos_table_io.xlsx", sheetName = "Sheet1", col.names = TRUE, row.names = FALSE, append = FALSE)


```

Using curling brackets within the query searches for an exact match to the phrase "Arctic Data Center". Plos does not allow for this type of search so we have to exclude it or the code won't run.

```{r}



res3 <- ft_search(query = "{Arctic Data Center}", from=c('crossref','arxiv', 'biorxiv', 'europmc'), limit = 100)

#For some reason I can't pull article titles from Biorxiv but I can get doi's

biorxiv_articles_exact <- ft_get(res3$biorxiv$data$doi[1:21])

biorxiv_titles_exact <- pub_chunks(fulltext::ft_collect(biorxiv_articles_exact), sections = c("doi", "title", "authors"))

biorxiv_table_exact <- unlist(biorxiv_titles_exact$biorxiv[1:21])

View(biorxiv_table_exact)


write.xlsx(biorxiv_table_exact, file = "biorxiv_table_exact.xlsx", sheetName = "Sheet1", 
  col.names = TRUE, row.names = FALSE, append = FALSE)

```









